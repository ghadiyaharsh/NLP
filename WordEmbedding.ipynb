{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33759a66",
   "metadata": {},
   "source": [
    "## Implement Word Embedding using Word2Vec, GloVe, and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cf37d",
   "metadata": {},
   "source": [
    "### What are Word Embeddings?\n",
    "\n",
    "Words are symbols → computers cannot directly understand them.\n",
    "\n",
    "Old techniques (BoW / TF-IDF) only count words, but they:\n",
    "\n",
    "Ignore meaning.\n",
    "\n",
    "Cannot capture relationships (e.g., “cat” and “dog” are both animals).\n",
    "\n",
    "Word embeddings = numerical vectors where similar words are closer in space.\n",
    "\n",
    "They capture semantic meaning and relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286e022",
   "metadata": {},
   "source": [
    "### 1. Word2Vec (by Google, 2013)\n",
    "\n",
    "Learns embeddings by predicting context of words.\n",
    "\n",
    "Two architectures:\n",
    "\n",
    "CBOW (Continuous Bag of Words): Predict target word from context.\n",
    "\n",
    "Skip-Gram: Predict context words from target word.\n",
    "\n",
    "Example: \"king - man + woman ≈ queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed1960",
   "metadata": {},
   "source": [
    "### 2. GloVe (Global Vectors, by Stanford, 2014)\n",
    "\n",
    "Pre-trained embeddings from large text (Wikipedia, Twitter).\n",
    "\n",
    "Uses word co-occurrence statistics.\n",
    "\n",
    "Advantage: Ready-to-use, very accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949b33d",
   "metadata": {},
   "source": [
    "### 3. FastText (by Facebook, 2016)\n",
    "\n",
    "Improves Word2Vec by considering subwords (character n-grams).\n",
    "\n",
    "Can generate embeddings for unseen/misspelled words.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"playing\" = play + ing\n",
    "\n",
    "\"player\" = play + er\n",
    "→ both share the root play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c724e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word vector for 'learning':\n",
      "[ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
      " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
      " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
      " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
      "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
      " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
      " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
      "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
      "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
      "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
      "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
      " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
      " -1.90201886e-02  9.01175756e-03]\n",
      "\n",
      "Top similar words to 'learning':\n",
      "[('meaning', 0.19010192155838013), ('processing', 0.0449172779917717), ('word', -0.010146019048988819), ('i', -0.014475265517830849), ('nlp', -0.023209011182188988), ('language', -0.04407211393117905), ('semantic', -0.044128309935331345), ('deep', -0.09488877654075623), ('embeddings', -0.12279320508241653), ('is', -0.15025532245635986)]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec learns meaning of words based on context.\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text corpus\n",
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Word embeddings capture semantic meaning\",\n",
    "    \"Machine learning is fun and powerful\",\n",
    "    \"I love deep learning and NLP\"\n",
    "]\n",
    "\n",
    "# Preprocess (tokenization)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,   # dimensions of word vector\n",
    "    window=5,         # context window\n",
    "    min_count=1,      # ignore words with less frequency\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Print word embedding\n",
    "print(\"\\nWord vector for 'learning':\")\n",
    "print(model.wv['learning'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nTop similar words to 'learning':\")\n",
    "print(model.wv.most_similar('learning'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
