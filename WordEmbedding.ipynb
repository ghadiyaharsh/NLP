{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33759a66",
   "metadata": {},
   "source": [
    "## Implement Word Embedding using Word2Vec, GloVe, and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cf37d",
   "metadata": {},
   "source": [
    "### What are Word Embeddings?\n",
    "\n",
    "Words are symbols → computers cannot directly understand them.\n",
    "\n",
    "Old techniques (BoW / TF-IDF) only count words, but they:\n",
    "\n",
    "Ignore meaning.\n",
    "\n",
    "Cannot capture relationships (e.g., “cat” and “dog” are both animals).\n",
    "\n",
    "Word embeddings = numerical vectors where similar words are closer in space.\n",
    "\n",
    "They capture semantic meaning and relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286e022",
   "metadata": {},
   "source": [
    "### 1. Word2Vec (by Google, 2013)\n",
    "\n",
    "Learns embeddings by predicting context of words.\n",
    "\n",
    "Two architectures:\n",
    "\n",
    "CBOW (Continuous Bag of Words): Predict target word from context.\n",
    "\n",
    "Skip-Gram: Predict context words from target word.\n",
    "\n",
    "Example: \"king - man + woman ≈ queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed1960",
   "metadata": {},
   "source": [
    "### 2. GloVe (Global Vectors, by Stanford, 2014)\n",
    "\n",
    "Pre-trained embeddings from large text (Wikipedia, Twitter).\n",
    "\n",
    "Uses word co-occurrence statistics.\n",
    "\n",
    "Advantage: Ready-to-use, very accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949b33d",
   "metadata": {},
   "source": [
    "### 3. FastText (by Facebook, 2016)\n",
    "\n",
    "Improves Word2Vec by considering subwords (character n-grams).\n",
    "\n",
    "Can generate embeddings for unseen/misspelled words.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"playing\" = play + ing\n",
    "\n",
    "\"player\" = play + er\n",
    "→ both share the root play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a5369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/24.0 MB 1.4 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 1.3/24.0 MB 1.8 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.8/24.0 MB 1.9 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 2.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 4.2/24.0 MB 2.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.0/24.0 MB 2.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 3.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.1/24.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.1/24.0 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.4/24.0 MB 3.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.3/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.0 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.3/24.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.0 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 19.9/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.2/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.8/24.0 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.1/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/15.5 MB 5.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.6/15.5 MB 6.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.5 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.5 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.2/15.5 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.7/15.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/45.9 MB 8.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 3.1/45.9 MB 8.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.2/45.9 MB 8.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.8/45.9 MB 8.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 8.9/45.9 MB 8.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 10.5/45.9 MB 8.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 12.1/45.9 MB 8.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 13.9/45.9 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 16.0/45.9 MB 8.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 17.8/45.9 MB 8.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 19.7/45.9 MB 8.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 21.5/45.9 MB 8.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 23.3/45.9 MB 8.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 25.2/45.9 MB 8.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 27.0/45.9 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 28.0/45.9 MB 8.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.9/45.9 MB 8.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 30.4/45.9 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.7/45.9 MB 8.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 33.3/45.9 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.9/45.9 MB 7.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.7/45.9 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 38.5/45.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.4/45.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.2/45.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.5/45.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/45.9 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 7.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'nlp': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "Most similar to 'language': [('nlp', 0.21617141366004944), ('processing', 0.04468921571969986), ('natural', 0.015029968693852425), ('embeddings', 0.0019510718993842602), ('learning', -0.03284316137433052), ('i', -0.04568909481167793), ('love', -0.0742427185177803), ('word', -0.09326908737421036), ('uses', -0.09575342386960983), ('are', -0.10513807833194733)]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "\n",
    "%pip install gensim\n",
    "from gensim.models import Word2Vec #importing the word2vec model from gensim library\n",
    "\n",
    "sentences=[[\"i\",\"love\",\"natural\",\"language\",\"processing\"],\n",
    "             [\"word\",\"embeddings\",\"are\",\"powerful\"],\n",
    "             [\"machine\",\"learning\",\"uses\",\"nlp\"]\n",
    "    \n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, workers=4) \n",
    "#we are creating the word2vec model with specified parameters which is sentences, vector size, window size, min count and number of workers. \n",
    "#meaning of this parameters are as follows:\n",
    "#sentences: list of sentences where each sentence is a list of words\n",
    "#vector_size: dimensionality of the word vectors dimension means the number of features we want to represent each word with\n",
    "#window: maximum distance between the current and predicted word within a sentence\n",
    "#min_count: ignores all words with total frequency lower than this\n",
    "#workers: number of worker threads to train the model\n",
    "\n",
    "print(\"Vector for 'nlp':\",model.wv['nlp'])\n",
    "print(\"Most similar to 'language':\", model.wv.most_similar('language'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171d4cd",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cae94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73cd420",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m             embeddings_index[word] \u001b[38;5;241m=\u001b[39m coefs \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_index\n\u001b[1;32m---> 11\u001b[0m glove_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove.6B.100d.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, glove_embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m10\u001b[39m])        \n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mload_glove\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_glove\u001b[39m(file_path):\n\u001b[0;32m      2\u001b[0m     embeddings_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m             values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:],dtypes=\"float32\")\n",
    "            embeddings_index[word] = coefs \n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove(\"glove.6B.100d.txt\")\n",
    "print(\"Vector for 'computer':\", glove_embeddings[\"computer\"][:10])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ab2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'nlp': [ 2.0917966e-03  2.8625538e-03 -1.7106148e-03 -5.1267720e-03\n",
      "  4.5366893e-03  2.5565268e-03  4.0376661e-03  2.8793167e-04\n",
      " -1.5643670e-03 -3.3923306e-03  1.0642613e-03  8.3877929e-03\n",
      "  1.6975831e-03  6.2881815e-03  1.1331455e-04  1.0461973e-03\n",
      "  4.7462885e-04  2.8110214e-03 -5.0762063e-03 -3.7041609e-03\n",
      " -1.3197183e-03 -4.0831654e-03 -7.4531804e-03 -2.8413215e-03\n",
      " -6.2535927e-03 -2.7256496e-03  2.0961852e-03  9.4518560e-04\n",
      "  2.9183957e-03 -3.3928468e-03  7.6779211e-03 -6.4904192e-03\n",
      "  1.7368069e-05 -6.8469131e-03 -1.0086624e-02  1.7397588e-03\n",
      " -7.6452931e-03 -2.7514505e-03 -4.9427070e-04 -3.8641447e-03\n",
      "  1.2573808e-03  5.5128452e-03 -3.9473148e-03  1.5680708e-03\n",
      "  3.3348035e-05 -8.9609314e-04  8.2994846e-04  5.9184490e-04\n",
      " -4.5531004e-04 -4.1969633e-04]\n",
      "Vector for unseen word 'nlping': [-1.4982434e-03  1.4012682e-03  6.6021406e-03  3.0075915e-03\n",
      " -2.1311657e-03  1.3356046e-03  5.2717333e-03  2.7976613e-03\n",
      " -1.7927448e-03  1.0274828e-03 -4.9013095e-03 -3.3298125e-03\n",
      " -1.8316704e-03  8.2001317e-04 -2.2330342e-03 -6.8432870e-03\n",
      "  1.6912217e-03  1.1915992e-03 -4.8289066e-03 -4.6876245e-03\n",
      " -2.7064392e-03 -3.8035118e-03 -8.1339683e-03  5.7092085e-03\n",
      " -2.8827572e-03  3.1079103e-03  2.8552890e-03 -8.7522238e-04\n",
      "  4.1515008e-03 -4.8432671e-03  5.9148371e-03 -1.1730333e-03\n",
      "  1.7080456e-06 -2.2778561e-04 -2.8967236e-03 -2.8539836e-04\n",
      " -3.5845067e-03 -3.6029464e-03  3.5983394e-03  5.5222423e-04\n",
      "  8.1112422e-03  3.2110820e-03  5.5678032e-04 -6.3588992e-03\n",
      " -1.9056137e-03 -2.6229350e-03 -2.9644261e-03  5.1654119e-04\n",
      "  8.6355227e-05 -5.1834816e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [[\"natural\",\"language\",\"processing\"],\n",
    "             [\"machine\",\"learning\",\"and\",\"nlp\"],\n",
    "             [\"word\",\"embeddings\",\"capture\",\"meaning\"]]\n",
    "\n",
    "ft_model = FastText(sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "print(\"Vector for 'nlp':\", ft_model.wv['nlp'])\n",
    "print(\"Vector for unseen word 'nlping':\", ft_model.wv['nlping'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
